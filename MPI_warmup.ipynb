{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORaYX9kMkLnqAa4xKfe/O5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarathiSoundarya/MPI_Codes/blob/main/MPI_warmup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hello World Program**"
      ],
      "metadata": {
        "id": "dNZXehZdfWYK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnTIMdXgYnGR",
        "outputId": "90dc1ffb-5d6f-4ce4-f0ac-ffd68d891a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting helloworld.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile helloworld.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  int rank, size; //size is the number of processors\n",
        "  MPI_Init(&argc,&argv);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "  printf(\"Helloo from rank %d of %d processes\\newline\",rank,size);\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc helloworld.c -o mpi_hello\n",
        "mpirun --allow-run-as-root -np 4 ./mpi_hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPfJ7syKcl0T",
        "outputId": "3da9e139-ac15-49fd-c0a0-a0d63dcbcb25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helloo from rank 0 of 4 processes\n",
            "Helloo from rank 1 of 4 processes\n",
            "Helloo from rank 3 of 4 processes\n",
            "Helloo from rank 2 of 4 processes\n",
            "ewlineewlineewlineewline"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab Exercises**<br /><br />\n",
        "**Question 1** Greetings example where all processes with ranks greater than 0 send their rank and size to the process with rank 0 for printing."
      ],
      "metadata": {
        "id": "ZyPj5JpTiZer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques1.c\n",
        "#include<mpi.h>\n",
        "#include<stdio.h>\n",
        "#include<string.h>\n",
        "#include<stdlib.h>\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "  int rank,size,src,dest;\n",
        "  int message[2];\n",
        "  MPI_Status status;\n",
        "  MPI_Init(&argc,&argv);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "  if(size==1)\n",
        "  {\n",
        "    printf(\"This example requires more than one process to execute\");\n",
        "    MPI_Finalize();\n",
        "    exit(0);\n",
        "  }\n",
        "  if (rank!=0)\n",
        "  {\n",
        "    message[0]=rank;\n",
        "    message[1]=size;\n",
        "    dest=0;   \n",
        "    MPI_Send(message,2,MPI_INT,dest,0,MPI_COMM_WORLD);\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "   for(src=1;src<size;src++)\n",
        "   {\n",
        "     MPI_Recv(message,2,MPI_INT,src,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n",
        "     printf(\"Hello fromm process %d of %d\\n\",message[0],message[1]);\n",
        "   } \n",
        "  }\n",
        " MPI_Finalize();\n",
        " return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAbcqd-afUZf",
        "outputId": "aa806d7d-fec8-4338-fd94-1dccbd73da84"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques1.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques1.c -o ques1\n",
        "mpirun --allow-run-as-root -np 4 ./ques1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDENyc8akTjF",
        "outputId": "bf9a50d6-e068-45d1-a25c-6fc87a40ebcf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello fromm process 1 of 4\n",
            "Hello fromm process 2 of 4\n",
            "Hello fromm process 3 of 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "Create a vector of size 4 and assign random values on the root process. Write a funtion for Broadcast of vector from root process to the other processes. (run on 4 processes)"
      ],
      "metadata": {
        "id": "SmvA2CKaoNDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques2.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<string.h>\n",
        "#include<stdlib.h>\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        " \n",
        "int rank,size;\n",
        "MPI_Init(&argc ,&argv ) ;\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank) ; // get the rank of this process\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size) ; //get the size of no of processors\n",
        "int *a;\n",
        "a=malloc(4*sizeof(int));\n",
        "for(int i=0;i<4;i++)\n",
        "{\n",
        "a[i]=0;\n",
        "}\n",
        "if(rank==0)\n",
        "{\n",
        "for(int i=0;i<4;i++)\n",
        "{\n",
        "a[i]=i;\n",
        "}\n",
        "}\n",
        "printf( \"Pre−Broadcast: on %d process is \\n\" , rank ) ;\n",
        "for(int i=0;i<4;i++)\n",
        "{\n",
        "printf(\" %d, \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Barrier(MPI_COMM_WORLD);\n",
        "MPI_Bcast(a,4,MPI_INT,0,MPI_COMM_WORLD);\n",
        "printf(\"\\n\");\n",
        "printf( \"Postbroadcast: on %d process is \\n\" , rank ) ;\n",
        "for(int i=0;i<4;i++)\n",
        "{\n",
        "printf(\" %d, \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Finalize();\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn04tagmo89h",
        "outputId": "3ec51e8c-0636-43be-ef60-d0ffbcc17f32"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques2.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques2.c -o ques2\n",
        "mpirun --allow-run-as-root -np 4 ./ques2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLA1nGbovTrt",
        "outputId": "7a8a88da-b5c4-4062-a081-0d6a07de4143"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre−Broadcast: on 0 process is \n",
            "Pre−Broadcast: on 3 process is \n",
            " 0,  0,  0,  0, \n",
            " 0,  1,  2,  3, \n",
            "Pre−Broadcast: on 1 process is \n",
            " 0,  0,  0,  0, \n",
            "Pre−Broadcast: on 2 process is \n",
            " 0,  0,  0,  0, \n",
            "\n",
            "Postbroadcast: on 0 process is \n",
            "\n",
            "Postbroadcast: on 1 process is \n",
            " 0,  1,  2,  3, \n",
            "\n",
            "Postbroadcast: on 2 process is \n",
            " 0,  1,  2,  3, \n",
            " 0,  1,  2,  3, \n",
            "\n",
            "Postbroadcast: on 3 process is \n",
            " 0,  1,  2,  3, \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3** \n",
        "Create a vector of size 8 and assign random values on the root process. Write\n",
        "a funtion for Scatter of vector of size 2 from root process to the other processes.\n",
        "<br/> MPI_Bcast sends the same piece of data to all processes while MPI_Scatter sends chunks of an array to different processes."
      ],
      "metadata": {
        "id": "9OnA3iAStSoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques3.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<string.h>\n",
        "#include<stdlib.h>\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        " \n",
        "int rank,size;\n",
        "MPI_Init(&argc ,&argv ) ;\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank) ; // get the rank of this process\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size) ; //get the size of no of processors\n",
        "int *a;\n",
        "a=malloc(8*sizeof(int));\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "a[i]=0;\n",
        "}\n",
        "if(rank==0)\n",
        "{\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "a[i]=i;\n",
        "}\n",
        "}\n",
        "printf( \"Pre−Broadcast: on %d process is \\n\" , rank ) ;\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "printf(\" %d, \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Barrier(MPI_COMM_WORLD);\n",
        "MPI_Scatter(a,2,MPI_INT,a,2,MPI_INT,0,MPI_COMM_WORLD);\n",
        "printf(\"\\n\");\n",
        "printf( \"Postbroadcast: on %d process is \\n\" , rank ) ;\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "printf(\" %d, \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Finalize();\n",
        "free(a);\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ8OsZbewkZ_",
        "outputId": "b1962e17-6de8-4858-9e62-befa50ba6feb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques3.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques3.c -o ques3\n",
        "mpirun --allow-run-as-root -np 4 ./ques3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk7Y305-wkqd",
        "outputId": "dc83622d-56dd-45ba-e837-f0abd04ec07a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre−Broadcast: on 0 process is \n",
            " 0,  1,  2,  3,  4,  5,  6,  7, \n",
            "Pre−Broadcast: on 3 process is \n",
            " 0,  0,  0,  0,  0,  0,  0,  0, \n",
            "Pre−Broadcast: on 2 process is \n",
            " 0,  0,  0,  0,  0,  0,  0,  0, \n",
            "Pre−Broadcast: on 1 process is \n",
            " 0,  0,  0,  0,  0,  0,  0,  0, \n",
            "\n",
            "Postbroadcast: on 0 process is \n",
            " 0,  1,  2,  3,  4,  5,  6,  7, \n",
            "\n",
            "Postbroadcast: on 3 process is \n",
            " 6,  7,  0,  0,  0,  0,  0,  0, \n",
            "\n",
            "Postbroadcast: on 2 process is \n",
            " 4,  5,  0,  0,  0,  0,  0,  0, \n",
            "\n",
            "Postbroadcast: on 1 process is \n",
            " 2,  3,  0,  0,  0,  0,  0,  0, \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques3b.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "int rank,size;\n",
        "MPI_Init(&argc,&argv);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "int *a,*b;\n",
        "a=malloc(8*sizeof(int));\n",
        "b=malloc(8*sizeof(int));\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "a[i]=0;\n",
        "b[i]=0;\n",
        "}\n",
        "if(rank==0)\n",
        "{\n",
        "for(int i=0;i<8;i++)\n",
        "{ \n",
        "a[i]=i;\n",
        "}\n",
        "}\n",
        "printf(\"pre-broadcast on process %d:\\n\",rank);\n",
        "for(int i=0;i<8;i++)\n",
        "{\n",
        "\tprintf(\"%d  \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Barrier(MPI_COMM_WORLD);\n",
        "MPI_Scatter(a,2,MPI_INT,b,2,MPI_INT,0,MPI_COMM_WORLD);\n",
        "printf(\"\\npost-broadcast on process %d:\\n\",rank);\n",
        "for(int i=0;i<8;i++) \n",
        "{\n",
        "printf(\"%d \",b[i]);\n",
        "}\n",
        "printf(\"\\n\")\n",
        "free(a);\n",
        "MPI_Finalize();\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eds41dAVyz7g",
        "outputId": "f99f6f16-9f6e-47e0-8021-bfe79a04a24a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques3b.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques3b.c -o ques3b\n",
        "mpirun --allow-run-as-root -np 4 ./ques3b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dryug38zMUs",
        "outputId": "0352901a-f000-4319-9761-f8ffed79be2c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kques3b.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kques3b.c:39:1:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ before ‘\u001b[01m\u001b[Kfree\u001b[m\u001b[K’\n",
            " \u001b[01;31m\u001b[Kfree\u001b[m\u001b[K(a);\n",
            " \u001b[01;31m\u001b[K^~~~\u001b[m\u001b[K\n",
            "pre-broadcast on process 1:\n",
            "pre-broadcast on process 3:\n",
            "0  0  0  0  0  0  0  0  \n",
            "0  0  0  0  0  0  0  0  \n",
            "pre-broadcast on process 2:\n",
            "0  0  0  0  0  0  0  0  \n",
            "pre-broadcast on process 0:\n",
            "0  1  2  3  4  5  6  7  \n",
            "\n",
            "\n",
            "post-broadcast on process 0:\n",
            "\n",
            "post-broadcast on process 2:\n",
            "\n",
            "post-broadcast on process 3:\n",
            "post-broadcast on process 1:\n",
            "2 3 0 0 0 0 0 0 4 5 0 0 0 0 0 0 6 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "Create a vector of size 4 and assign random values on all processes. Write a\n",
        "funtion for Gather of vector of size 16 from all other processes to the root process."
      ],
      "metadata": {
        "id": "3jhH7jZztS3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques4.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "int main(int argc,char** argv)\n",
        "{\n",
        "int rank,size,*a,*b;\n",
        "a=malloc(4*sizeof(int));\n",
        "MPI_Init(&argc,&argv);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "printf(\"Pre gather array on process:%d\\n\",rank);\n",
        "for(int i=0;i<4;i++)\n",
        "{\n",
        "a[i]=rand()%(20+rank);\n",
        "printf(\"%d \",a[i]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "MPI_Barrier(MPI_COMM_WORLD);\n",
        "b=malloc(16*sizeof(int));\n",
        "for(int i=0;i<16;i++)\n",
        "{\n",
        "\tb[i]=0;\n",
        "}\n",
        "\t\n",
        "MPI_Gather(a,4,MPI_INT,b,4,MPI_INT,0,MPI_COMM_WORLD);\n",
        "printf(\"post gather array of b on process %d is:\",rank);\n",
        "for(int i=0;i<16;i++)\n",
        "\t{ printf(\"%d \",b[i]);}\n",
        "printf(\"\\npost gather array a on process %d is:\",rank);\n",
        "for(int i=0;i<4;i++)\n",
        "\t{ printf(\"%d \",a[i]);}\n",
        "printf(\"\\n\");\n",
        "free(a);\n",
        "free(b);\n",
        "MPI_Finalize();\n",
        "return 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EcxOblmzrjb",
        "outputId": "a6675eff-b5aa-4fb2-8c9e-52e1a7e202fe"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques4.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques4.c -o ques4\n",
        "mpirun --allow-run-as-root -np 4 ./ques4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYfyJYow3OME",
        "outputId": "402293a3-c9e0-4677-a52b-06c96da07768"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre gather array on process:2\n",
            "Pre gather array on process:0\n",
            "3 6 17 15 \n",
            "17 10 17 13 \n",
            "Pre gather array on process:1\n",
            "1 4 9 19 \n",
            "Pre gather array on process:3\n",
            "11 0 6 2 \n",
            "post gather array of b on process 3 is:0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "post gather array a on process 3 is:11 0 6 2 \n",
            "post gather array of b on process 1 is:0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "post gather array a on process 1 is:1 4 9 19 \n",
            "post gather array of b on process 2 is:0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
            "post gather array a on process 2 is:17 10 17 13 \n",
            "post gather array of b on process 0 is:3 6 17 15 1 4 9 19 17 10 17 13 11 0 6 2 \n",
            "post gather array a on process 0 is:3 6 17 15 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**\n",
        "Computing average of numbers (create 10,000 random numbers) with MPI_Scatter and MPI_Gather.\n",
        "Here we create an array on the root rank and scatter it across all ranks. Compute the local average of the scattered array in each of the ranks and then sum it up using MPIReduce. "
      ],
      "metadata": {
        "id": "2gbIdklftTAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques5.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "\n",
        "double localaverage(int *b,int localn,int totaln)\n",
        "{\n",
        "double average=0;\n",
        "for(int i=0;i<localn;i++)\n",
        "{average+=b[i];}\n",
        "average/=totaln;\n",
        "return average;\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc,char ** argv)\n",
        "{\n",
        "int *a,*b,rank,size,localn,localbeg,localend,startindex;\n",
        "double localav,totalav=0,actual_sum=0;\n",
        "MPI_Init(&argc,&argv);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "\n",
        "if(rank==0)\n",
        "{\n",
        "a=malloc(100*sizeof(int));\n",
        "srand((time(0)));\n",
        "for(int i=0;i<100;i++)\n",
        "{\n",
        "a[i]=rand()%100; \n",
        "actual_sum+=a[i];\n",
        "}\n",
        "actual_sum/=100;\n",
        "printf(\"actual average is: %f\\n\",actual_sum);\n",
        "}\n",
        "\n",
        "localn=100/size;\n",
        "b=malloc(localn*sizeof(int));\n",
        "for(int i=0;i<localn;i++)\n",
        "{\n",
        "\tb[i]=0;\n",
        "}\n",
        "MPI_Scatter(a,localn,MPI_INT,b,localn,MPI_INT,0,MPI_COMM_WORLD);\n",
        "localav=localaverage(b,localn,100);\n",
        "printf(\"localaverage on rank:%d is %f\\n\",rank,localav);\n",
        "MPI_Barrier(MPI_COMM_WORLD);\n",
        "\n",
        "MPI_Reduce(&localav,&totalav,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "\n",
        "if(rank==0)\n",
        "{\n",
        "\tprintf(\"total average is:%f\",totalav);\n",
        "}\n",
        "\n",
        "MPI_Finalize();\n",
        "return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9dXClnS4Soe",
        "outputId": "ba3ed7f9-0bb3-4185-8912-af9b057067d4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques5.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques5.c -o ques5\n",
        "mpirun --allow-run-as-root -np 4 ./ques5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R66JL4kL4Szn",
        "outputId": "be02ed77-abb0-4f46-f94f-72e44ca16a0a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual average is: 48.280000\n",
            "localaverage on rank:0 is 14.810000\n",
            "localaverage on rank:3 is 10.830000\n",
            "localaverage on rank:2 is 13.410000\n",
            "localaverage on rank:1 is 9.230000\n",
            "total average is:48.280000"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**\n",
        "Calculation of $\\pi$ using Leibniz Formula, $\\pi=4\\cdot\\sum_{i=0}^{n}\\frac{(-1)^{i}}{2i+1}$."
      ],
      "metadata": {
        "id": "k2JZ-7VxtTUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques6.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "#include<math.h>\n",
        "int main(int argc,char** argv)\n",
        "{\n",
        "\tdouble localsum=0,totalsum,tsum=1,n=100,k;\n",
        "  int a,b,localn,rank,size;\n",
        "\tMPI_Init(&argc,&argv);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "\tlocaln=n/size;\n",
        "\ta=(localn*rank)+1;\n",
        "\tb=(localn)+(a-1);\n",
        "  for(int i=a;i<=b;i++)\n",
        "  {\n",
        "    k=(double)1/((2*i)+1);\n",
        "    if(i%2==0)\n",
        "    {localsum+=k;}\n",
        "    else\n",
        "    {localsum-=k;}\n",
        "  }\n",
        "  printf(\"a is:%d,b is:%d,local sum of rank %d is:%f\\n\",a,b,rank,localsum);\n",
        "  MPI_Reduce(&localsum,&totalsum,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "  if(rank==0)\n",
        "  {\n",
        "    totalsum+=1;\n",
        "    totalsum*=4;\n",
        "    printf(\"totalsum is:%f\",totalsum);\n",
        "  }\n",
        "  MPI_Finalize();\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKw_KhN4M-W-",
        "outputId": "09b1c412-af73-4cdb-c3d9-86721eb4a218"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques6.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques6.c -o ques6\n",
        "mpirun --allow-run-as-root -np 4 ./ques6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPePH2W7KI42",
        "outputId": "7022dae7-b782-4855-980d-340529d26ea3"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a is:26,b is:50,local sum of rank 1 is:0.014513\n",
            "a is:76,b is:100,local sum of rank 3 is:0.005765\n",
            "a is:1,b is:25,local sum of rank 0 is:-0.224214\n",
            "a is:51,b is:75,local sum of rank 2 is:-0.008191\n",
            "totalsum is:3.151493"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**\n",
        "Create a two random vectors of size 10,00,000 and computes the dot product ofthese two vectors. Display the CPU timings."
      ],
      "metadata": {
        "id": "PC692NsWtTJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques7.c\n",
        "\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "\n",
        "double locdot(int *a,int *b,int ln,double lprod)\n",
        "{\n",
        "\tlprod=0;\n",
        "\tfor(int i=0;i<ln;i++)\n",
        "\t{\n",
        "\t\tlprod+=a[i]*b[i];\n",
        "\t}\n",
        "\treturn lprod;\n",
        "}\n",
        "\n",
        "int main(int argc,char ** argv)\n",
        "{\n",
        "int rank,size,*a,*b,*la,*lb,n=20,localn,process,sum=0;\n",
        "double localdot,totaldot;\n",
        "MPI_Status status;\n",
        "MPI_Init(&argc,&argv);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "if(rank==0)\n",
        "{\n",
        "  a=malloc(n*sizeof(int));\n",
        "  b=malloc(n*sizeof(int));\n",
        "  for(int i=0;i<n;i++)\n",
        "  {\n",
        "    a[i]=rand()%100;\n",
        "    b[i]=rand()%100;\n",
        "    sum+=(a[i]*b[i]);\n",
        "  }\n",
        "  printf(\"sequential sum is:%d\\n\",sum);\n",
        "}\n",
        "localn=n/size;\n",
        "la=malloc(localn*sizeof(int));\n",
        "lb=malloc(localn*sizeof(int));\n",
        "MPI_Scatter(a,localn,MPI_INT,la,localn,MPI_INT,0,MPI_COMM_WORLD);\n",
        "MPI_Scatter(b,localn,MPI_INT,lb,localn,MPI_INT,0,MPI_COMM_WORLD);\n",
        "localdot=locdot(la,lb,localn,localdot);\n",
        "MPI_Reduce(&localdot,&totaldot,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "if(rank==0)\n",
        "{\n",
        "printf(\"totaldot product in parallel:%f\",totaldot);\n",
        "}\n",
        "MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSx4X7oIfcaO",
        "outputId": "090f9f9c-2920-4631-d663-23a4c080aa18"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques7.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques7.c -o ques7\n",
        "mpirun --allow-run-as-root -np 4 ./ques7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuSxtQD_jcuI",
        "outputId": "b1cd6dd5-2bb8-424e-d52c-edfb0a85c255"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequential sum is:53292\n",
            "totaldot product in parallel:53292.000000"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**\n",
        "Create a random vector of size 100,000 integers and distribute it among 4 processors. Find the Minimum, Maximum and Sum of the numbers in the vector. Also,\n",
        "find the sum of each local vector on that rank and find their product. Morover,\n",
        "find the maximum and the minimum along thier localtion in the vector. Print the\n",
        "CPU time of all the functions and observe the scalability over the processors."
      ],
      "metadata": {
        "id": "_XNjXSrJtTfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ques8.c\n",
        "#include<stdio.h>\n",
        "#include<mpi.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "int main(int argc,char** argv)\n",
        "{\n",
        "\tint *a,*b,n=20,rank,size,localn,lsum=0,lmax,lmin,sum,max,min,s=0;\n",
        "\tMPI_Init(&argc,&argv);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&size);\n",
        "\tdouble t1,t2;\n",
        "\tMPI_Barrier(MPI_COMM_WORLD);\n",
        "\tt1=MPI_Wtime();\n",
        "\tif(rank==0)\n",
        "\t{\n",
        "\t\ta=malloc(20*sizeof(int));\n",
        "\t\tsrand(time(0));\n",
        "\t\tfor(int i=0;i<n;i++)\n",
        "\t\t{\n",
        "\t\t\ta[i]=rand()%100;\n",
        "\t\t\ts+=a[i];\n",
        "\t\t\tprintf(\" %d\",a[i]);\n",
        "\t\t}\n",
        "\t}\n",
        "\tlocaln=n/size;\n",
        "\tb=malloc(localn*sizeof(int));\n",
        "\tMPI_Scatter(a,localn,MPI_INT,b,localn,MPI_INT,0,MPI_COMM_WORLD);\n",
        "\tlmax=b[0];\n",
        "\tlmin=b[0];\n",
        "\tlsum=0;\n",
        "\tfor(int i=0;i<localn;i++)\n",
        "\t{\n",
        "\t\tif(b[i]>lmax)\n",
        "\t\t{lmax=b[i];}\n",
        "\tif(b[i]<lmin)\n",
        "\t{lmin=b[i];}\n",
        "lsum+=b[i];\n",
        "\t}\n",
        "\tMPI_Reduce(&lmax,&max,1,MPI_INT,MPI_MAX,0,MPI_COMM_WORLD);\n",
        "\tMPI_Reduce(&lmin,&min,1,MPI_INT,MPI_MIN,0,MPI_COMM_WORLD);\n",
        "\tMPI_Reduce(&lsum,&sum,1,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "\tif(rank==0)\n",
        "\t{\n",
        "\t\tprintf(\"\\nmax:%d,min: %d,sum:%d \\n\",max,min,sum);\n",
        "\t}\n",
        "\tMPI_Barrier(MPI_COMM_WORLD);\n",
        "\tt2=MPI_Wtime();\n",
        "\tprintf(\"computational time for rank %d is:%f\\n\",rank,t2-t1);\n",
        "  MPI_Finalize();\n",
        "\treturn 0;\n",
        "}\n",
        "\t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlNj4_yllWgP",
        "outputId": "e8ecef41-4721-485d-d6c0-b97ee19c0da3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ques8.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mpicc ques8.c -o ques8\n",
        "mpirun --allow-run-as-root -np 4 ./ques8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmyvfWDol-4h",
        "outputId": "22f54a51-7ed8-4b61-ee05-2a738d0b80b9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 7 11 79 98 26 94 57 9 56 2 82 33 32 34 24 16 22 36 51 52\n",
            "max:98,min: 2,sum:821 \n",
            "computational time for rank 0 is:0.000720\n",
            "computational time for rank 2 is:0.000730\n",
            "computational time for rank 1 is:0.000721\n",
            "computational time for rank 3 is:0.000867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}